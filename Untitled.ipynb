{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6c455c-d9f9-4594-8698-496bcd01bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf4668e-86c7-4330-b82b-fe48407b34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for training and validation datasets\n",
    "train_dir = r'C:\\Users\\sushil kumar gupta\\Desktop\\kuch bhi\\train'\n",
    "validation_dir = r'C:\\Users\\sushil kumar gupta\\Desktop\\kuch bhi\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5586cf69-1fd2-4b01-9368-407614684957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training parameters\n",
    "num_classes = 36\n",
    "image_size = (224, 224)  # VGG16's expected input size\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def8ec42-b857-42f3-bdf6-027e77c10b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation and Preprocessing for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7daba7-9e57-426a-b843-27c92278f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for the validation set (no augmentation, only rescaling)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e042e7-f7d6-410c-9539-516cccf1772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model, excluding the fully connected layer (include_top=False)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf3f5a5-d56b-4fd8-873c-cfe799cb09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a custom classification head\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Global Average Pooling instead of Flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the layers of the base VGG16 model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de809ff2-d940-4fab-b061-7e5ae89a81e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3115 images belonging to 36 classes.\n",
      "Found 359 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c89e60-79fd-49ce-aa07-91aab786ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Indices (from training set): {'apple': 0, 'banana': 1, 'beetroot': 2, 'bell pepper': 3, 'cabbage': 4, 'capsicum': 5, 'carrot': 6, 'cauliflower': 7, 'chilli pepper': 8, 'corn': 9, 'cucumber': 10, 'eggplant': 11, 'garlic': 12, 'ginger': 13, 'grapes': 14, 'jalepeno': 15, 'kiwi': 16, 'lemon': 17, 'lettuce': 18, 'mango': 19, 'onion': 20, 'orange': 21, 'paprika': 22, 'pear': 23, 'peas': 24, 'pineapple': 25, 'pomegranate': 26, 'potato': 27, 'raddish': 28, 'soy beans': 29, 'spinach': 30, 'sweetcorn': 31, 'sweetpotato': 32, 'tomato': 33, 'turnip': 34, 'watermelon': 35}\n"
     ]
    }
   ],
   "source": [
    "# Display the class indices\n",
    "class_indices = train_generator.class_indices\n",
    "print(\"Class Indices (from training set):\", class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1b6e01-8854-4c42-a481-3adb57c57c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to Class Mapping: {0: 'apple', 1: 'banana', 2: 'beetroot', 3: 'bell pepper', 4: 'cabbage', 5: 'capsicum', 6: 'carrot', 7: 'cauliflower', 8: 'chilli pepper', 9: 'corn', 10: 'cucumber', 11: 'eggplant', 12: 'garlic', 13: 'ginger', 14: 'grapes', 15: 'jalepeno', 16: 'kiwi', 17: 'lemon', 18: 'lettuce', 19: 'mango', 20: 'onion', 21: 'orange', 22: 'paprika', 23: 'pear', 24: 'peas', 25: 'pineapple', 26: 'pomegranate', 27: 'potato', 28: 'raddish', 29: 'soy beans', 30: 'spinach', 31: 'sweetcorn', 32: 'sweetpotato', 33: 'tomato', 34: 'turnip', 35: 'watermelon'}\n"
     ]
    }
   ],
   "source": [
    "# Reverse the class indices to get a mapping from index to class name\n",
    "index_to_class = {v: k for k, v in class_indices.items()}\n",
    "print(\"Index to Class Mapping:\", index_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168eda9-1be9-40b6-bde9-f08afa006d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushil kumar gupta\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1304s\u001b[0m 13s/step - accuracy: 0.1335 - loss: 3.3552 - val_accuracy: 0.4545 - val_loss: 2.0892\n",
      "Epoch 2/10\n",
      "\u001b[1m 1/97\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:49\u001b[0m 11s/step - accuracy: 0.3125 - loss: 2.5685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.3125 - loss: 2.5685 - val_accuracy: 0.2857 - val_loss: 2.3195\n",
      "Epoch 3/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1334s\u001b[0m 14s/step - accuracy: 0.3641 - loss: 2.3422 - val_accuracy: 0.5795 - val_loss: 1.5157\n",
      "Epoch 4/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 2.2469 - val_accuracy: 0.7143 - val_loss: 0.8872\n",
      "Epoch 5/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1332s\u001b[0m 14s/step - accuracy: 0.4729 - loss: 1.9376 - val_accuracy: 0.7102 - val_loss: 1.1339\n",
      "Epoch 6/10\n",
      "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - accuracy: 0.6562 - loss: 1.1849 - val_accuracy: 0.5714 - val_loss: 1.3558\n",
      "Epoch 7/10\n",
      "\u001b[1m86/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m2:13\u001b[0m 12s/step - accuracy: 0.5223 - loss: 1.7188"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.n // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9d10a-2536-4397-84c6-cba9097d5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "model.save('my_vgg16_model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa4ff5-82df-4526-8b01-df9adfb98e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model for prediction\n",
    "loaded_model = tf.keras.models.load_model('my_vgg16_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d863e-916e-40a5-913e-9deafdb974b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the image for prediction\n",
    "def prepare_image(img_path):\n",
    "    # Load the image and resize it to (224, 224) for VGG16\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)  # Apply VGG16 preprocessing\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fe7c3-5aaf-47a6-915e-2792ee018e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the input image (replace with your image path)\n",
    "input_image_path = r'C:\\Users\\sushil kumar gupta\\Desktop\\kiwi.jpg'\n",
    "prepared_image = prepare_image(input_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f2282-5d7e-4c72-a2a7-35d95c93d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = loaded_model.predict(prepared_image)\n",
    "\n",
    "# Get the predicted class index\n",
    "predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "# Get the confidence score (highest probability)\n",
    "confidence_score = np.max(predictions) * 100\n",
    "\n",
    "# Map the predicted class index to the class name\n",
    "predicted_label = index_to_class[predicted_class]\n",
    "\n",
    "# Print the predicted class and confidence\n",
    "print(f\"The predicted class is: {predicted_label}\")\n",
    "print(f\"Confidence: {confidence_score:.2f}%\")\n",
    "\n",
    "# Print the predicted probabilities for all classes\n",
    "print(\"\\nConfidence for each class:\")\n",
    "for i, class_name in index_to_class.items():\n",
    "    print(f\"{class_name}: {predictions[0][i]*100:.2f}%\")\n",
    "\n",
    "# Optional: Visualize the training history (accuracy and loss)\n",
    "def plot_training_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(epochs)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "    plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75d572-46f2-422b-9504-df7d207d2544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddbeff-452a-49a9-a895-ac62bca625ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
